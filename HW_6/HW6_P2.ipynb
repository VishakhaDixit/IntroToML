{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PzGnZZ6-u4rQ"
   },
   "outputs": [],
   "source": [
    "# Vishakha Dixit\n",
    "# 801265288\n",
    "# HW-6 Problem-2 Solutions\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56PwIQwak7bp",
    "outputId": "cfccc31c-8da9-4439-9739-10c6c0176668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X3-V0be8vBh3",
    "outputId": "660e39ff-9da2-47fb-9190-3df53914f09c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Part-1\n",
    "\n",
    "from torchvision import datasets\n",
    "data_path = '/content/drive/MyDrive/ML_HW/data'\n",
    "cifar10 = datasets.CIFAR10(data_path, train = True, \n",
    "                           download = True, \n",
    "            transform =  transforms.Compose([transforms.ToTensor(), \n",
    "                            transforms.Normalize((0.4915, 0.4823, 0.4468), \n",
    "                                            (0.2470, 0.2435, 0.2616))]))\n",
    "cifar10_val = datasets.CIFAR10(data_path, train = False, download = True, \n",
    "                    transform = transforms.Compose([transforms.ToTensor(), \n",
    "                            transforms.Normalize((0.4915, 0.4823, 0.4468), \n",
    "                                            (0.2470, 0.2435, 0.2616))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ELbo72aZvEmH"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Conv2d(3,16, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16,8, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Linear(8 * 8 * 8, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9--QS31Yvhum"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "    self.act1 = nn.Tanh()\n",
    "    self.pool1 = nn.MaxPool2d(2)\n",
    "    self.conv2 = nn.Conv2d(16,8, kernel_size=3, padding=1)\n",
    "    self.act2 = nn.Tanh()\n",
    "    self.pool2 = nn.MaxPool2d(2)\n",
    "    self.fc1 = nn.Linear(8*8*8,32)\n",
    "    self.act3 = nn.Tanh()\n",
    "    self.fc2 = nn.Linear(32,10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.pool1(self.act1(self.conv1(x)))\n",
    "    out = self.pool2(self.act2(self.conv2(out)))\n",
    "    out = out.view(-1, 8*8*8)\n",
    "    out = self.act3(self.fc1(out))\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MrANG0C0vlL2"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "    self.conv2 = nn.Conv2d(16,8, kernel_size=3, padding=1)\n",
    "    self.fc1 = nn.Linear(8*8*8, 32)\n",
    "    self.fc2 = nn.Linear(32, 10)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "    out = F.max_pool2d(torch.tanh(self.conv2(out)),2)\n",
    "    out = out.view(-1, 8*8*8)\n",
    "    out = torch.tanh(self.fc1(out))\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0AjEIpxOvnom",
    "outputId": "aea9c99a-7d39-4ded-b285-5156f2d76f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda:0.\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda:0') if torch.cuda.is_available() \n",
    "          else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PCVU-Ib-vrBG"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, \n",
    "                  train_loader):\n",
    "  for epoch in range(1, n_epochs +1):\n",
    "    loss_train = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "      outputs = model(imgs.to('cuda:0'))\n",
    "      loss = loss_fn(outputs.to('cuda:0'), \n",
    "                     labels.to('cuda:0'))\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      loss_train += loss.item()\n",
    "    if epoch == 1 or epoch % 1 == 0:\n",
    "      print('{} Epoch {}, Training Loss {}'.format(datetime.datetime.now(), \n",
    "                                    epoch, loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWiWZ27ivt22",
    "outputId": "adda131e-3ad9-48a4-aa2b-079515221a4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-04 01:27:59.121769 Epoch 1, Training Loss 2.050850319892854\n",
      "2022-12-04 01:28:10.858518 Epoch 2, Training Loss 1.7718762310264666\n",
      "2022-12-04 01:28:23.067879 Epoch 3, Training Loss 1.591920509210328\n",
      "2022-12-04 01:28:34.991023 Epoch 4, Training Loss 1.4869809280271116\n",
      "2022-12-04 01:28:48.265787 Epoch 5, Training Loss 1.4133399459712035\n",
      "2022-12-04 01:28:59.784604 Epoch 6, Training Loss 1.3533931693152699\n",
      "2022-12-04 01:29:10.918246 Epoch 7, Training Loss 1.300388334428563\n",
      "2022-12-04 01:29:22.198832 Epoch 8, Training Loss 1.256114103376408\n",
      "2022-12-04 01:29:33.497897 Epoch 9, Training Loss 1.2152485180541377\n",
      "2022-12-04 01:29:44.776556 Epoch 10, Training Loss 1.1835584632880853\n",
      "2022-12-04 01:29:55.809318 Epoch 11, Training Loss 1.1575147327407243\n",
      "2022-12-04 01:30:07.031878 Epoch 12, Training Loss 1.1340423022847042\n",
      "2022-12-04 01:30:18.469723 Epoch 13, Training Loss 1.1165185524984393\n",
      "2022-12-04 01:30:29.956917 Epoch 14, Training Loss 1.0983067761601695\n",
      "2022-12-04 01:30:41.370199 Epoch 15, Training Loss 1.0830043629002388\n",
      "2022-12-04 01:30:52.665772 Epoch 16, Training Loss 1.0672331175688283\n",
      "2022-12-04 01:31:05.351120 Epoch 17, Training Loss 1.0523442819600215\n",
      "2022-12-04 01:31:16.610651 Epoch 18, Training Loss 1.0385163630670904\n",
      "2022-12-04 01:31:27.965010 Epoch 19, Training Loss 1.0271937754910316\n",
      "2022-12-04 01:31:39.341345 Epoch 20, Training Loss 1.0175744718145532\n",
      "2022-12-04 01:31:50.746820 Epoch 21, Training Loss 1.007271516353578\n",
      "2022-12-04 01:32:02.236089 Epoch 22, Training Loss 0.9980447405134626\n",
      "2022-12-04 01:32:13.692496 Epoch 23, Training Loss 0.9859088006074471\n",
      "2022-12-04 01:32:25.214176 Epoch 24, Training Loss 0.9801071717611054\n",
      "2022-12-04 01:32:36.728306 Epoch 25, Training Loss 0.9712503128649329\n",
      "2022-12-04 01:32:48.063263 Epoch 26, Training Loss 0.9656516039158072\n",
      "2022-12-04 01:32:59.463077 Epoch 27, Training Loss 0.9574368682206439\n",
      "2022-12-04 01:33:10.855478 Epoch 28, Training Loss 0.9489453283264814\n",
      "2022-12-04 01:33:23.392051 Epoch 29, Training Loss 0.9408761185148488\n",
      "2022-12-04 01:33:34.780689 Epoch 30, Training Loss 0.9364944747494309\n",
      "2022-12-04 01:33:46.294404 Epoch 31, Training Loss 0.9269597894700287\n",
      "2022-12-04 01:33:57.556362 Epoch 32, Training Loss 0.9203884609977303\n",
      "2022-12-04 01:34:09.008982 Epoch 33, Training Loss 0.9157218244069677\n",
      "2022-12-04 01:34:20.584615 Epoch 34, Training Loss 0.9097070269420019\n",
      "2022-12-04 01:34:32.010479 Epoch 35, Training Loss 0.9047091625383138\n",
      "2022-12-04 01:34:43.408497 Epoch 36, Training Loss 0.8998636608690862\n",
      "2022-12-04 01:34:54.789396 Epoch 37, Training Loss 0.8932229540384639\n",
      "2022-12-04 01:35:06.248707 Epoch 38, Training Loss 0.8883033757624419\n",
      "2022-12-04 01:35:17.608662 Epoch 39, Training Loss 0.8836972849905643\n",
      "2022-12-04 01:35:29.200448 Epoch 40, Training Loss 0.8784067543876141\n",
      "2022-12-04 01:35:41.871303 Epoch 41, Training Loss 0.8733965660757421\n",
      "2022-12-04 01:35:53.320862 Epoch 42, Training Loss 0.8696267117014931\n",
      "2022-12-04 01:36:04.743889 Epoch 43, Training Loss 0.8633464718871104\n",
      "2022-12-04 01:36:16.181642 Epoch 44, Training Loss 0.8608094390548403\n",
      "2022-12-04 01:36:27.590051 Epoch 45, Training Loss 0.8565291127432948\n",
      "2022-12-04 01:36:38.856799 Epoch 46, Training Loss 0.8524767593349643\n",
      "2022-12-04 01:36:50.088318 Epoch 47, Training Loss 0.8475006023026488\n",
      "2022-12-04 01:37:01.469687 Epoch 48, Training Loss 0.8427421680039457\n",
      "2022-12-04 01:37:12.710727 Epoch 49, Training Loss 0.8390189199267751\n",
      "2022-12-04 01:37:24.142594 Epoch 50, Training Loss 0.8354905397462113\n",
      "2022-12-04 01:37:35.519116 Epoch 51, Training Loss 0.8311898186612312\n",
      "2022-12-04 01:37:46.950346 Epoch 52, Training Loss 0.8293717567954222\n",
      "2022-12-04 01:37:59.580923 Epoch 53, Training Loss 0.8228302454704519\n",
      "2022-12-04 01:38:10.930843 Epoch 54, Training Loss 0.8196494905540096\n",
      "2022-12-04 01:38:22.225967 Epoch 55, Training Loss 0.8161382020434456\n",
      "2022-12-04 01:38:33.580386 Epoch 56, Training Loss 0.8137208102914073\n",
      "2022-12-04 01:38:44.909525 Epoch 57, Training Loss 0.8074333612113962\n",
      "2022-12-04 01:38:56.329657 Epoch 58, Training Loss 0.8065903345337304\n",
      "2022-12-04 01:39:07.721504 Epoch 59, Training Loss 0.8020163779444707\n",
      "2022-12-04 01:39:19.130369 Epoch 60, Training Loss 0.7994582554720857\n",
      "2022-12-04 01:39:30.623804 Epoch 61, Training Loss 0.7962305131546982\n",
      "2022-12-04 01:39:42.094794 Epoch 62, Training Loss 0.7933771732975455\n",
      "2022-12-04 01:39:53.474266 Epoch 63, Training Loss 0.7898641466484655\n",
      "2022-12-04 01:40:04.726242 Epoch 64, Training Loss 0.7857646368958456\n",
      "2022-12-04 01:40:17.140232 Epoch 65, Training Loss 0.7852786108660881\n",
      "2022-12-04 01:40:28.502885 Epoch 66, Training Loss 0.7807845525119615\n",
      "2022-12-04 01:40:39.913619 Epoch 67, Training Loss 0.7770744889517269\n",
      "2022-12-04 01:40:51.299168 Epoch 68, Training Loss 0.7755974763265961\n",
      "2022-12-04 01:41:02.715531 Epoch 69, Training Loss 0.7724235069263926\n",
      "2022-12-04 01:41:14.022866 Epoch 70, Training Loss 0.7707711294331514\n",
      "2022-12-04 01:41:25.355796 Epoch 71, Training Loss 0.7678005181233901\n",
      "2022-12-04 01:41:36.740069 Epoch 72, Training Loss 0.7659777280543466\n",
      "2022-12-04 01:41:48.049543 Epoch 73, Training Loss 0.7621717654606875\n",
      "2022-12-04 01:41:59.312207 Epoch 74, Training Loss 0.7602532573444459\n",
      "2022-12-04 01:42:10.467869 Epoch 75, Training Loss 0.7571630915793617\n",
      "2022-12-04 01:42:21.643285 Epoch 76, Training Loss 0.754355505871041\n",
      "2022-12-04 01:42:33.979837 Epoch 77, Training Loss 0.7532998425576388\n",
      "2022-12-04 01:42:45.266787 Epoch 78, Training Loss 0.7484304347001683\n",
      "2022-12-04 01:42:56.563726 Epoch 79, Training Loss 0.7467762371691902\n",
      "2022-12-04 01:43:07.981163 Epoch 80, Training Loss 0.746159465416618\n",
      "2022-12-04 01:43:19.304430 Epoch 81, Training Loss 0.7434617238657554\n",
      "2022-12-04 01:43:30.686479 Epoch 82, Training Loss 0.7405346287485889\n",
      "2022-12-04 01:43:41.857368 Epoch 83, Training Loss 0.7380822688493582\n",
      "2022-12-04 01:43:53.043743 Epoch 84, Training Loss 0.7365901207603762\n",
      "2022-12-04 01:44:04.317020 Epoch 85, Training Loss 0.7335713490119675\n",
      "2022-12-04 01:44:15.483989 Epoch 86, Training Loss 0.732387475078673\n",
      "2022-12-04 01:44:26.794341 Epoch 87, Training Loss 0.7295762292869256\n",
      "2022-12-04 01:44:38.195960 Epoch 88, Training Loss 0.7262726950523494\n",
      "2022-12-04 01:44:50.043318 Epoch 89, Training Loss 0.7265444037783176\n",
      "2022-12-04 01:45:01.905453 Epoch 90, Training Loss 0.7231791739344902\n",
      "2022-12-04 01:45:13.169941 Epoch 91, Training Loss 0.7225828811030863\n",
      "2022-12-04 01:45:24.393786 Epoch 92, Training Loss 0.7198250269722146\n",
      "2022-12-04 01:45:35.744239 Epoch 93, Training Loss 0.7176182294440696\n",
      "2022-12-04 01:45:47.064339 Epoch 94, Training Loss 0.7160338638231273\n",
      "2022-12-04 01:45:58.488322 Epoch 95, Training Loss 0.71370243328764\n",
      "2022-12-04 01:46:09.774622 Epoch 96, Training Loss 0.7123430307259035\n",
      "2022-12-04 01:46:21.030859 Epoch 97, Training Loss 0.7112152521567576\n",
      "2022-12-04 01:46:32.493582 Epoch 98, Training Loss 0.7072049952147866\n",
      "2022-12-04 01:46:43.780020 Epoch 99, Training Loss 0.7064952059932377\n",
      "2022-12-04 01:46:54.928650 Epoch 100, Training Loss 0.7052985241498484\n",
      "2022-12-04 01:47:06.539843 Epoch 101, Training Loss 0.7032986886589728\n",
      "2022-12-04 01:47:18.815443 Epoch 102, Training Loss 0.7020935550751284\n",
      "2022-12-04 01:47:30.187208 Epoch 103, Training Loss 0.6991911320125356\n",
      "2022-12-04 01:47:41.404089 Epoch 104, Training Loss 0.698214851033962\n",
      "2022-12-04 01:47:52.596743 Epoch 105, Training Loss 0.6971806907059287\n",
      "2022-12-04 01:48:03.922114 Epoch 106, Training Loss 0.6952936591394722\n",
      "2022-12-04 01:48:15.170581 Epoch 107, Training Loss 0.6923610238772829\n",
      "2022-12-04 01:48:26.518550 Epoch 108, Training Loss 0.6911209320549465\n",
      "2022-12-04 01:48:37.856287 Epoch 109, Training Loss 0.6897622911673983\n",
      "2022-12-04 01:48:49.116476 Epoch 110, Training Loss 0.6877722446723362\n",
      "2022-12-04 01:49:00.360145 Epoch 111, Training Loss 0.6868413075461717\n",
      "2022-12-04 01:49:11.525414 Epoch 112, Training Loss 0.6868532385743792\n",
      "2022-12-04 01:49:22.639043 Epoch 113, Training Loss 0.6850889109056014\n",
      "2022-12-04 01:49:35.185585 Epoch 114, Training Loss 0.682441847708524\n",
      "2022-12-04 01:49:46.329856 Epoch 115, Training Loss 0.68105137245277\n",
      "2022-12-04 01:49:57.517496 Epoch 116, Training Loss 0.6820273657741449\n",
      "2022-12-04 01:50:08.791138 Epoch 117, Training Loss 0.6772691155486095\n",
      "2022-12-04 01:50:19.862961 Epoch 118, Training Loss 0.677790866788391\n",
      "2022-12-04 01:50:31.009803 Epoch 119, Training Loss 0.6758217030321546\n",
      "2022-12-04 01:50:42.217849 Epoch 120, Training Loss 0.6732811503626807\n",
      "2022-12-04 01:50:53.440613 Epoch 121, Training Loss 0.6732652427442848\n",
      "2022-12-04 01:51:04.753331 Epoch 122, Training Loss 0.6713974561608965\n",
      "2022-12-04 01:51:16.135706 Epoch 123, Training Loss 0.6686247994222909\n",
      "2022-12-04 01:51:27.396452 Epoch 124, Training Loss 0.6676279596431786\n",
      "2022-12-04 01:51:38.796137 Epoch 125, Training Loss 0.6655677412553211\n",
      "2022-12-04 01:51:51.245399 Epoch 126, Training Loss 0.6652828180957633\n",
      "2022-12-04 01:52:02.543247 Epoch 127, Training Loss 0.6642659955164966\n",
      "2022-12-04 01:52:13.957714 Epoch 128, Training Loss 0.6623638675874456\n",
      "2022-12-04 01:52:25.247388 Epoch 129, Training Loss 0.6631419682289328\n",
      "2022-12-04 01:52:36.532579 Epoch 130, Training Loss 0.6597536112112767\n",
      "2022-12-04 01:52:47.582940 Epoch 131, Training Loss 0.6590197300133498\n",
      "2022-12-04 01:52:58.684837 Epoch 132, Training Loss 0.6576482637611496\n",
      "2022-12-04 01:53:09.796575 Epoch 133, Training Loss 0.6561234808715103\n",
      "2022-12-04 01:53:20.782609 Epoch 134, Training Loss 0.653605193395139\n",
      "2022-12-04 01:53:31.703702 Epoch 135, Training Loss 0.6526823663117026\n",
      "2022-12-04 01:53:42.803742 Epoch 136, Training Loss 0.6522968869913569\n",
      "2022-12-04 01:53:53.835743 Epoch 137, Training Loss 0.6521338242703997\n",
      "2022-12-04 01:54:06.001074 Epoch 138, Training Loss 0.6495944815675926\n",
      "2022-12-04 01:54:16.983736 Epoch 139, Training Loss 0.6478758948233426\n",
      "2022-12-04 01:54:28.028683 Epoch 140, Training Loss 0.6484344221289505\n",
      "2022-12-04 01:54:39.248968 Epoch 141, Training Loss 0.6460223423931605\n",
      "2022-12-04 01:54:50.304263 Epoch 142, Training Loss 0.6446720571316722\n",
      "2022-12-04 01:55:01.063946 Epoch 143, Training Loss 0.6424281815509967\n",
      "2022-12-04 01:55:11.965916 Epoch 144, Training Loss 0.6430451890925313\n",
      "2022-12-04 01:55:23.080892 Epoch 145, Training Loss 0.6409667495952542\n",
      "2022-12-04 01:55:33.868761 Epoch 146, Training Loss 0.6391803908073689\n",
      "2022-12-04 01:55:45.562529 Epoch 147, Training Loss 0.6392239062758662\n",
      "2022-12-04 01:55:56.962322 Epoch 148, Training Loss 0.6383691162556944\n",
      "2022-12-04 01:56:08.573142 Epoch 149, Training Loss 0.635865864584513\n",
      "2022-12-04 01:56:20.360018 Epoch 150, Training Loss 0.6358844926366416\n",
      "2022-12-04 01:56:33.613089 Epoch 151, Training Loss 0.6344853642651492\n",
      "2022-12-04 01:56:45.251906 Epoch 152, Training Loss 0.6335846091170445\n",
      "2022-12-04 01:56:56.833316 Epoch 153, Training Loss 0.6317707282274275\n",
      "2022-12-04 01:57:09.538799 Epoch 154, Training Loss 0.6304731611019511\n",
      "2022-12-04 01:57:22.089533 Epoch 155, Training Loss 0.6300120808736748\n",
      "2022-12-04 01:57:34.618350 Epoch 156, Training Loss 0.6283884357156047\n",
      "2022-12-04 01:57:46.784103 Epoch 157, Training Loss 0.6278370158446719\n",
      "2022-12-04 01:57:58.980036 Epoch 158, Training Loss 0.6263060256495805\n",
      "2022-12-04 01:58:11.024843 Epoch 159, Training Loss 0.6252751330585431\n",
      "2022-12-04 01:58:23.000657 Epoch 160, Training Loss 0.6240991452313445\n",
      "2022-12-04 01:58:34.423274 Epoch 161, Training Loss 0.6238197406082202\n",
      "2022-12-04 01:58:47.328043 Epoch 162, Training Loss 0.6218881520163982\n",
      "2022-12-04 01:58:58.460168 Epoch 163, Training Loss 0.622243128850332\n",
      "2022-12-04 01:59:09.477236 Epoch 164, Training Loss 0.6207189351091604\n",
      "2022-12-04 01:59:20.697986 Epoch 165, Training Loss 0.6195614212156867\n",
      "2022-12-04 01:59:31.769713 Epoch 166, Training Loss 0.6181635315842031\n",
      "2022-12-04 01:59:42.872505 Epoch 167, Training Loss 0.61858515372819\n",
      "2022-12-04 01:59:53.978296 Epoch 168, Training Loss 0.6162267445450853\n",
      "2022-12-04 02:00:04.840745 Epoch 169, Training Loss 0.6167595878891323\n",
      "2022-12-04 02:00:15.891803 Epoch 170, Training Loss 0.6156237842253102\n",
      "2022-12-04 02:00:27.173070 Epoch 171, Training Loss 0.6137373825854353\n",
      "2022-12-04 02:00:38.305186 Epoch 172, Training Loss 0.6131954158053678\n",
      "2022-12-04 02:00:50.151141 Epoch 173, Training Loss 0.6126443532193103\n",
      "2022-12-04 02:01:01.232309 Epoch 174, Training Loss 0.61051512023677\n",
      "2022-12-04 02:01:13.490460 Epoch 175, Training Loss 0.6096656806862263\n",
      "2022-12-04 02:01:24.632136 Epoch 176, Training Loss 0.6100631338327437\n",
      "2022-12-04 02:01:35.631228 Epoch 177, Training Loss 0.6093983100655743\n",
      "2022-12-04 02:01:46.674046 Epoch 178, Training Loss 0.6086883365421953\n",
      "2022-12-04 02:01:57.726903 Epoch 179, Training Loss 0.606893997820442\n",
      "2022-12-04 02:02:08.666493 Epoch 180, Training Loss 0.6058566797038784\n",
      "2022-12-04 02:02:19.418924 Epoch 181, Training Loss 0.6044236543919425\n",
      "2022-12-04 02:02:30.292570 Epoch 182, Training Loss 0.6036971412656252\n",
      "2022-12-04 02:02:41.090370 Epoch 183, Training Loss 0.6036271000152353\n",
      "2022-12-04 02:02:51.948578 Epoch 184, Training Loss 0.6005318943420639\n",
      "2022-12-04 02:03:03.366689 Epoch 185, Training Loss 0.601337181828211\n",
      "2022-12-04 02:03:14.312326 Epoch 186, Training Loss 0.6020503972116333\n",
      "2022-12-04 02:03:26.530993 Epoch 187, Training Loss 0.5987929099279902\n",
      "2022-12-04 02:03:37.556859 Epoch 188, Training Loss 0.5992753682538982\n",
      "2022-12-04 02:03:48.574299 Epoch 189, Training Loss 0.5973502639157083\n",
      "2022-12-04 02:03:59.655407 Epoch 190, Training Loss 0.5961350852342517\n",
      "2022-12-04 02:04:10.716524 Epoch 191, Training Loss 0.5951999399591895\n",
      "2022-12-04 02:04:21.710935 Epoch 192, Training Loss 0.5947976324640577\n",
      "2022-12-04 02:04:32.985601 Epoch 193, Training Loss 0.5935958492786378\n",
      "2022-12-04 02:04:44.040907 Epoch 194, Training Loss 0.5945146780108552\n",
      "2022-12-04 02:04:55.782377 Epoch 195, Training Loss 0.5918697899640979\n",
      "2022-12-04 02:05:06.694803 Epoch 196, Training Loss 0.592131751920561\n",
      "2022-12-04 02:05:17.560218 Epoch 197, Training Loss 0.5916158211276964\n",
      "2022-12-04 02:05:28.631127 Epoch 198, Training Loss 0.5903922343040671\n",
      "2022-12-04 02:05:39.585739 Epoch 199, Training Loss 0.5895736801731007\n",
      "2022-12-04 02:05:51.808652 Epoch 200, Training Loss 0.5898857723797679\n",
      "2022-12-04 02:06:02.771369 Epoch 201, Training Loss 0.5901742434257742\n",
      "2022-12-04 02:06:13.750221 Epoch 202, Training Loss 0.5865636369608858\n",
      "2022-12-04 02:06:24.683735 Epoch 203, Training Loss 0.5880513551366299\n",
      "2022-12-04 02:06:35.773892 Epoch 204, Training Loss 0.5852249924407895\n",
      "2022-12-04 02:06:46.786105 Epoch 205, Training Loss 0.5846955557079876\n",
      "2022-12-04 02:06:57.747100 Epoch 206, Training Loss 0.5858428779694126\n",
      "2022-12-04 02:07:08.667975 Epoch 207, Training Loss 0.5833870866490752\n",
      "2022-12-04 02:07:19.584140 Epoch 208, Training Loss 0.5836762310293935\n",
      "2022-12-04 02:07:30.625727 Epoch 209, Training Loss 0.5815039960014851\n",
      "2022-12-04 02:07:41.571961 Epoch 210, Training Loss 0.5792773112540355\n",
      "2022-12-04 02:07:52.491709 Epoch 211, Training Loss 0.5818834274702365\n",
      "2022-12-04 02:08:04.029642 Epoch 212, Training Loss 0.5799427450541645\n",
      "2022-12-04 02:08:15.272196 Epoch 213, Training Loss 0.5799539254220856\n",
      "2022-12-04 02:08:26.069138 Epoch 214, Training Loss 0.5784586215644236\n",
      "2022-12-04 02:08:37.089800 Epoch 215, Training Loss 0.5778249130605737\n",
      "2022-12-04 02:08:47.948655 Epoch 216, Training Loss 0.5765223699381284\n",
      "2022-12-04 02:08:58.831880 Epoch 217, Training Loss 0.5757673487943762\n",
      "2022-12-04 02:09:09.567468 Epoch 218, Training Loss 0.57423215075527\n",
      "2022-12-04 02:09:20.286975 Epoch 219, Training Loss 0.5753941625509116\n",
      "2022-12-04 02:09:31.008839 Epoch 220, Training Loss 0.5739603562809318\n",
      "2022-12-04 02:09:41.877984 Epoch 221, Training Loss 0.5737902004548046\n",
      "2022-12-04 02:09:52.679940 Epoch 222, Training Loss 0.5726615712237175\n",
      "2022-12-04 02:10:03.362839 Epoch 223, Training Loss 0.5710787324573073\n",
      "2022-12-04 02:10:14.164019 Epoch 224, Training Loss 0.5729892228722877\n",
      "2022-12-04 02:10:25.940545 Epoch 225, Training Loss 0.571733172325527\n",
      "2022-12-04 02:10:36.839107 Epoch 226, Training Loss 0.5704413894039896\n",
      "2022-12-04 02:10:47.648344 Epoch 227, Training Loss 0.5673791735678377\n",
      "2022-12-04 02:10:58.364635 Epoch 228, Training Loss 0.5714400039838098\n",
      "2022-12-04 02:11:09.214781 Epoch 229, Training Loss 0.5692479049077119\n",
      "2022-12-04 02:11:20.011483 Epoch 230, Training Loss 0.5678634719013254\n",
      "2022-12-04 02:11:30.847651 Epoch 231, Training Loss 0.5651055665882042\n",
      "2022-12-04 02:11:41.708640 Epoch 232, Training Loss 0.5672952846035628\n",
      "2022-12-04 02:11:52.585019 Epoch 233, Training Loss 0.5649857014951194\n",
      "2022-12-04 02:12:03.290557 Epoch 234, Training Loss 0.5648939802747248\n",
      "2022-12-04 02:12:14.105913 Epoch 235, Training Loss 0.5640635435538524\n",
      "2022-12-04 02:12:24.858371 Epoch 236, Training Loss 0.5639895139371648\n",
      "2022-12-04 02:12:35.890071 Epoch 237, Training Loss 0.5644641292979345\n",
      "2022-12-04 02:12:47.854818 Epoch 238, Training Loss 0.5606581339293428\n",
      "2022-12-04 02:12:58.767579 Epoch 239, Training Loss 0.5615396710765331\n",
      "2022-12-04 02:13:09.677233 Epoch 240, Training Loss 0.5612682738267553\n",
      "2022-12-04 02:13:20.434936 Epoch 241, Training Loss 0.560146291923645\n",
      "2022-12-04 02:13:31.221818 Epoch 242, Training Loss 0.5595769026623968\n",
      "2022-12-04 02:13:42.121110 Epoch 243, Training Loss 0.5591509171077967\n",
      "2022-12-04 02:13:52.854784 Epoch 244, Training Loss 0.557240207862976\n",
      "2022-12-04 02:14:03.658995 Epoch 245, Training Loss 0.558184940110692\n",
      "2022-12-04 02:14:14.520009 Epoch 246, Training Loss 0.5563109836462513\n",
      "2022-12-04 02:14:25.183133 Epoch 247, Training Loss 0.5562729233366144\n",
      "2022-12-04 02:14:35.920724 Epoch 248, Training Loss 0.5562793556457896\n",
      "2022-12-04 02:14:46.772957 Epoch 249, Training Loss 0.5557225346565247\n",
      "2022-12-04 02:14:57.551504 Epoch 250, Training Loss 0.5550962211302174\n",
      "2022-12-04 02:15:09.368596 Epoch 251, Training Loss 0.5548007863256937\n",
      "2022-12-04 02:15:20.186341 Epoch 252, Training Loss 0.5552519962305913\n",
      "2022-12-04 02:15:30.994829 Epoch 253, Training Loss 0.5545516661594591\n",
      "2022-12-04 02:15:42.016274 Epoch 254, Training Loss 0.5549287696933503\n",
      "2022-12-04 02:15:52.893731 Epoch 255, Training Loss 0.5536337953699214\n",
      "2022-12-04 02:16:03.531940 Epoch 256, Training Loss 0.5512760648184725\n",
      "2022-12-04 02:16:14.296577 Epoch 257, Training Loss 0.5524033194841327\n",
      "2022-12-04 02:16:24.926251 Epoch 258, Training Loss 0.5514033603889253\n",
      "2022-12-04 02:16:35.696569 Epoch 259, Training Loss 0.5521120806712934\n",
      "2022-12-04 02:16:46.693843 Epoch 260, Training Loss 0.5516881616524113\n",
      "2022-12-04 02:16:57.576362 Epoch 261, Training Loss 0.5486455618039422\n",
      "2022-12-04 02:17:08.294680 Epoch 262, Training Loss 0.5473359790451996\n",
      "2022-12-04 02:17:19.056118 Epoch 263, Training Loss 0.5481141212651187\n",
      "2022-12-04 02:17:30.837143 Epoch 264, Training Loss 0.5484832455130184\n",
      "2022-12-04 02:17:41.608047 Epoch 265, Training Loss 0.54552542467785\n",
      "2022-12-04 02:17:52.406887 Epoch 266, Training Loss 0.5455952282528134\n",
      "2022-12-04 02:18:03.044226 Epoch 267, Training Loss 0.5464795452097188\n",
      "2022-12-04 02:18:13.835110 Epoch 268, Training Loss 0.5463549486359062\n",
      "2022-12-04 02:18:24.566381 Epoch 269, Training Loss 0.5445045624166498\n",
      "2022-12-04 02:18:35.173353 Epoch 270, Training Loss 0.5458105213349432\n",
      "2022-12-04 02:18:45.959787 Epoch 271, Training Loss 0.5443335079094943\n",
      "2022-12-04 02:18:56.700936 Epoch 272, Training Loss 0.5435276648501302\n",
      "2022-12-04 02:19:07.382839 Epoch 273, Training Loss 0.5432383338051379\n",
      "2022-12-04 02:19:18.244525 Epoch 274, Training Loss 0.542484642332777\n",
      "2022-12-04 02:19:28.855779 Epoch 275, Training Loss 0.5412028011344278\n",
      "2022-12-04 02:19:39.638755 Epoch 276, Training Loss 0.5398279177906263\n",
      "2022-12-04 02:19:51.470641 Epoch 277, Training Loss 0.5418278303978693\n",
      "2022-12-04 02:20:02.110795 Epoch 278, Training Loss 0.5402485751320639\n",
      "2022-12-04 02:20:12.804813 Epoch 279, Training Loss 0.5384631505821977\n",
      "2022-12-04 02:20:23.522424 Epoch 280, Training Loss 0.54062216906139\n",
      "2022-12-04 02:20:34.223123 Epoch 281, Training Loss 0.5389612000387953\n",
      "2022-12-04 02:20:44.954869 Epoch 282, Training Loss 0.5376010167857875\n",
      "2022-12-04 02:20:55.854647 Epoch 283, Training Loss 0.5370533597819945\n",
      "2022-12-04 02:21:06.564038 Epoch 284, Training Loss 0.5384759440294007\n",
      "2022-12-04 02:21:17.294555 Epoch 285, Training Loss 0.5382256050929999\n",
      "2022-12-04 02:21:28.036851 Epoch 286, Training Loss 0.5364140642192358\n",
      "2022-12-04 02:21:38.772557 Epoch 287, Training Loss 0.5363607810205205\n",
      "2022-12-04 02:21:49.504687 Epoch 288, Training Loss 0.5360448752598994\n",
      "2022-12-04 02:22:00.309391 Epoch 289, Training Loss 0.5363196899442721\n",
      "2022-12-04 02:22:12.149740 Epoch 290, Training Loss 0.5355518729714177\n",
      "2022-12-04 02:22:22.855839 Epoch 291, Training Loss 0.535306453609558\n",
      "2022-12-04 02:22:33.587949 Epoch 292, Training Loss 0.5329817513294537\n",
      "2022-12-04 02:22:44.387339 Epoch 293, Training Loss 0.5327337000452345\n",
      "2022-12-04 02:22:55.239988 Epoch 294, Training Loss 0.5323245177793381\n",
      "2022-12-04 02:23:05.816637 Epoch 295, Training Loss 0.533557107610166\n",
      "2022-12-04 02:23:16.440643 Epoch 296, Training Loss 0.5308776431147705\n",
      "2022-12-04 02:23:27.175909 Epoch 297, Training Loss 0.5305814381755526\n",
      "2022-12-04 02:23:38.109569 Epoch 298, Training Loss 0.5297453700543364\n",
      "2022-12-04 02:23:48.914838 Epoch 299, Training Loss 0.5328673769331649\n",
      "2022-12-04 02:23:59.715615 Epoch 300, Training Loss 0.5292377106064116\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, \n",
    "                                           shuffle=True)\n",
    "\n",
    "model = Net().to('cuda:0')\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs =300,\n",
    "    optimizer= optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Yo6WIfldvylm"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, \n",
    "                    batch_size=64, shuffle=False)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, \n",
    "                    batch_size=64, shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "      correct = 0\n",
    "      total = 0\n",
    "    \n",
    "      with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "          outputs = model(imgs.to('cuda:0'))\n",
    "          _, predicted = torch.max(outputs.to('cuda:0'), \n",
    "                                   dim=1)\n",
    "          total += labels.shape[0]\n",
    "          correct += int((predicted.to('cuda:0') == \n",
    "                          labels.to('cuda:0')).sum())\n",
    "    \n",
    "      print(\"Accuracy {}: {:.2f}\".format(name, \n",
    "                                         correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "void60C8v2uu",
    "outputId": "2bc29d64-1a3d-4fc9-d1fd-4ff2112ad691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.82\n",
      "Accuracy val: 0.62\n"
     ]
    }
   ],
   "source": [
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "m5GtcoQnv5C-"
   },
   "outputs": [],
   "source": [
    "# Part-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pGtaPe0Lv67m"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Conv2d(3,16, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16,8, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8,4, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Linear(4*4*4, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qTM3yJsXv9fm",
    "outputId": "63f94a28-7f08-43d4-f75b-73aa77242737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-04 02:24:24.092688 Epoch 1, Training Loss 2.033293886715189\n",
      "2022-12-04 02:24:34.869699 Epoch 2, Training Loss 1.7438221730844443\n",
      "2022-12-04 02:24:45.781078 Epoch 3, Training Loss 1.5968979371478185\n",
      "2022-12-04 02:24:56.733606 Epoch 4, Training Loss 1.5085458702138623\n",
      "2022-12-04 02:25:07.628696 Epoch 5, Training Loss 1.4357301949540062\n",
      "2022-12-04 02:25:18.421438 Epoch 6, Training Loss 1.367181647616579\n",
      "2022-12-04 02:25:29.323309 Epoch 7, Training Loss 1.3099394624343004\n",
      "2022-12-04 02:25:40.229547 Epoch 8, Training Loss 1.2627357881697243\n",
      "2022-12-04 02:25:50.917148 Epoch 9, Training Loss 1.2238019455576796\n",
      "2022-12-04 02:26:01.534904 Epoch 10, Training Loss 1.188430308685888\n",
      "2022-12-04 02:26:12.245191 Epoch 11, Training Loss 1.1595776033066119\n",
      "2022-12-04 02:26:23.074453 Epoch 12, Training Loss 1.1321494075495873\n",
      "2022-12-04 02:26:33.680562 Epoch 13, Training Loss 1.1084413164107085\n",
      "2022-12-04 02:26:45.459593 Epoch 14, Training Loss 1.088182297432819\n",
      "2022-12-04 02:26:56.405973 Epoch 15, Training Loss 1.0702353280675991\n",
      "2022-12-04 02:27:07.178375 Epoch 16, Training Loss 1.05498906466967\n",
      "2022-12-04 02:27:17.914942 Epoch 17, Training Loss 1.0392394155797446\n",
      "2022-12-04 02:27:28.642567 Epoch 18, Training Loss 1.0256244861103994\n",
      "2022-12-04 02:27:39.426193 Epoch 19, Training Loss 1.0146283726862935\n",
      "2022-12-04 02:27:50.223327 Epoch 20, Training Loss 1.0014948434841908\n",
      "2022-12-04 02:28:01.122529 Epoch 21, Training Loss 0.9919225461495197\n",
      "2022-12-04 02:28:11.833446 Epoch 22, Training Loss 0.9819239602064538\n",
      "2022-12-04 02:28:22.662298 Epoch 23, Training Loss 0.9727662493810629\n",
      "2022-12-04 02:28:33.270933 Epoch 24, Training Loss 0.9644035726709439\n",
      "2022-12-04 02:28:43.992114 Epoch 25, Training Loss 0.9579795560111171\n",
      "2022-12-04 02:28:54.767063 Epoch 26, Training Loss 0.95193735870254\n",
      "2022-12-04 02:29:06.601881 Epoch 27, Training Loss 0.9427374509137\n",
      "2022-12-04 02:29:17.276635 Epoch 28, Training Loss 0.9356191743670217\n",
      "2022-12-04 02:29:27.922199 Epoch 29, Training Loss 0.9284939208756322\n",
      "2022-12-04 02:29:38.877629 Epoch 30, Training Loss 0.9236470845044421\n",
      "2022-12-04 02:29:49.793459 Epoch 31, Training Loss 0.9176292132081278\n",
      "2022-12-04 02:30:01.182260 Epoch 32, Training Loss 0.9115473914634237\n",
      "2022-12-04 02:30:12.052298 Epoch 33, Training Loss 0.9061243816867204\n",
      "2022-12-04 02:30:23.081083 Epoch 34, Training Loss 0.9009994345019235\n",
      "2022-12-04 02:30:34.000078 Epoch 35, Training Loss 0.8949077595072938\n",
      "2022-12-04 02:30:44.848795 Epoch 36, Training Loss 0.8908026035484451\n",
      "2022-12-04 02:30:55.674679 Epoch 37, Training Loss 0.8868643293142928\n",
      "2022-12-04 02:31:06.407495 Epoch 38, Training Loss 0.8823755122816471\n",
      "2022-12-04 02:31:17.213072 Epoch 39, Training Loss 0.8769682549759555\n",
      "2022-12-04 02:31:29.156814 Epoch 40, Training Loss 0.8729688266811468\n",
      "2022-12-04 02:31:40.052049 Epoch 41, Training Loss 0.8666601160450664\n",
      "2022-12-04 02:31:51.158391 Epoch 42, Training Loss 0.8629307313953214\n",
      "2022-12-04 02:32:02.016028 Epoch 43, Training Loss 0.8583731899785874\n",
      "2022-12-04 02:32:12.823356 Epoch 44, Training Loss 0.8549395621280231\n",
      "2022-12-04 02:32:23.713764 Epoch 45, Training Loss 0.8512309212666338\n",
      "2022-12-04 02:32:34.751877 Epoch 46, Training Loss 0.8479141035805577\n",
      "2022-12-04 02:32:45.704452 Epoch 47, Training Loss 0.8441069105168437\n",
      "2022-12-04 02:32:56.831594 Epoch 48, Training Loss 0.8408123533744032\n",
      "2022-12-04 02:33:07.694788 Epoch 49, Training Loss 0.8360697303129279\n",
      "2022-12-04 02:33:18.355727 Epoch 50, Training Loss 0.831983341509119\n",
      "2022-12-04 02:33:29.182290 Epoch 51, Training Loss 0.8303666465041583\n",
      "2022-12-04 02:33:39.995823 Epoch 52, Training Loss 0.8268524186538003\n",
      "2022-12-04 02:33:52.016597 Epoch 53, Training Loss 0.8227492237030087\n",
      "2022-12-04 02:34:02.932494 Epoch 54, Training Loss 0.8198632220630451\n",
      "2022-12-04 02:34:13.807853 Epoch 55, Training Loss 0.8156708470162224\n",
      "2022-12-04 02:34:24.658088 Epoch 56, Training Loss 0.812722719233969\n",
      "2022-12-04 02:34:35.498414 Epoch 57, Training Loss 0.8104423199544477\n",
      "2022-12-04 02:34:46.425433 Epoch 58, Training Loss 0.805978617323634\n",
      "2022-12-04 02:34:57.341911 Epoch 59, Training Loss 0.8021601082571327\n",
      "2022-12-04 02:35:08.184372 Epoch 60, Training Loss 0.8012189741634652\n",
      "2022-12-04 02:35:18.943574 Epoch 61, Training Loss 0.7975135453597969\n",
      "2022-12-04 02:35:29.666667 Epoch 62, Training Loss 0.7959946231235324\n",
      "2022-12-04 02:35:40.532324 Epoch 63, Training Loss 0.7923483976241573\n",
      "2022-12-04 02:35:51.456477 Epoch 64, Training Loss 0.7893397754720409\n",
      "2022-12-04 02:36:02.320038 Epoch 65, Training Loss 0.7859046939769974\n",
      "2022-12-04 02:36:14.222882 Epoch 66, Training Loss 0.7822682936020824\n",
      "2022-12-04 02:36:25.024006 Epoch 67, Training Loss 0.7810557820760381\n",
      "2022-12-04 02:36:35.914547 Epoch 68, Training Loss 0.7786238485818628\n",
      "2022-12-04 02:36:46.636633 Epoch 69, Training Loss 0.7752180870460428\n",
      "2022-12-04 02:36:57.670302 Epoch 70, Training Loss 0.772066648849441\n",
      "2022-12-04 02:37:08.694235 Epoch 71, Training Loss 0.7694167732582677\n",
      "2022-12-04 02:37:19.516274 Epoch 72, Training Loss 0.7676800088123288\n",
      "2022-12-04 02:37:30.407294 Epoch 73, Training Loss 0.7645580864242275\n",
      "2022-12-04 02:37:41.530563 Epoch 74, Training Loss 0.7617216184163642\n",
      "2022-12-04 02:37:52.522395 Epoch 75, Training Loss 0.7594668859106195\n",
      "2022-12-04 02:38:03.424032 Epoch 76, Training Loss 0.7575149646652933\n",
      "2022-12-04 02:38:14.316242 Epoch 77, Training Loss 0.7561907608948095\n",
      "2022-12-04 02:38:25.258127 Epoch 78, Training Loss 0.7520735762689424\n",
      "2022-12-04 02:38:37.279602 Epoch 79, Training Loss 0.7508308431300361\n",
      "2022-12-04 02:38:48.174051 Epoch 80, Training Loss 0.7479369909028568\n",
      "2022-12-04 02:38:59.166392 Epoch 81, Training Loss 0.7445480193552154\n",
      "2022-12-04 02:39:10.072873 Epoch 82, Training Loss 0.7429660546124134\n",
      "2022-12-04 02:39:20.877205 Epoch 83, Training Loss 0.7415092086700528\n",
      "2022-12-04 02:39:31.683123 Epoch 84, Training Loss 0.7390288692301191\n",
      "2022-12-04 02:39:42.638588 Epoch 85, Training Loss 0.7370955978741731\n",
      "2022-12-04 02:39:53.572983 Epoch 86, Training Loss 0.7352651729608131\n",
      "2022-12-04 02:40:04.464202 Epoch 87, Training Loss 0.730903652165552\n",
      "2022-12-04 02:40:15.423505 Epoch 88, Training Loss 0.7308545746766698\n",
      "2022-12-04 02:40:26.295109 Epoch 89, Training Loss 0.7275783636076066\n",
      "2022-12-04 02:40:37.068674 Epoch 90, Training Loss 0.7253576950801303\n",
      "2022-12-04 02:40:48.436429 Epoch 91, Training Loss 0.7230260364539788\n",
      "2022-12-04 02:40:59.994226 Epoch 92, Training Loss 0.7211709410866813\n",
      "2022-12-04 02:41:10.818986 Epoch 93, Training Loss 0.7189775134825036\n",
      "2022-12-04 02:41:21.506087 Epoch 94, Training Loss 0.7176216585404428\n",
      "2022-12-04 02:41:32.221909 Epoch 95, Training Loss 0.7158916699307044\n",
      "2022-12-04 02:41:43.123441 Epoch 96, Training Loss 0.7139543877995532\n",
      "2022-12-04 02:41:53.985704 Epoch 97, Training Loss 0.7124821516635168\n",
      "2022-12-04 02:42:04.841938 Epoch 98, Training Loss 0.7115470735770663\n",
      "2022-12-04 02:42:15.638903 Epoch 99, Training Loss 0.7091686149768512\n",
      "2022-12-04 02:42:26.438313 Epoch 100, Training Loss 0.7070368490636806\n",
      "2022-12-04 02:42:37.237969 Epoch 101, Training Loss 0.7052552822377066\n",
      "2022-12-04 02:42:48.165699 Epoch 102, Training Loss 0.7026559794726579\n",
      "2022-12-04 02:42:58.926603 Epoch 103, Training Loss 0.7021475767006959\n",
      "2022-12-04 02:43:10.649191 Epoch 104, Training Loss 0.6992716085727867\n",
      "2022-12-04 02:43:21.460877 Epoch 105, Training Loss 0.6973327456609063\n",
      "2022-12-04 02:43:32.372091 Epoch 106, Training Loss 0.6969815721673429\n",
      "2022-12-04 02:43:43.061353 Epoch 107, Training Loss 0.6954752003674007\n",
      "2022-12-04 02:43:53.998549 Epoch 108, Training Loss 0.6941569607199916\n",
      "2022-12-04 02:44:04.804543 Epoch 109, Training Loss 0.6905969355798438\n",
      "2022-12-04 02:44:15.553425 Epoch 110, Training Loss 0.6900067585508537\n",
      "2022-12-04 02:44:26.341244 Epoch 111, Training Loss 0.6865343092118993\n",
      "2022-12-04 02:44:37.065034 Epoch 112, Training Loss 0.6871718453324359\n",
      "2022-12-04 02:44:47.817772 Epoch 113, Training Loss 0.6850002013966251\n",
      "2022-12-04 02:44:58.588254 Epoch 114, Training Loss 0.6840774282393858\n",
      "2022-12-04 02:45:09.632624 Epoch 115, Training Loss 0.6820521124107454\n",
      "2022-12-04 02:45:20.487738 Epoch 116, Training Loss 0.6802474581219656\n",
      "2022-12-04 02:45:32.266478 Epoch 117, Training Loss 0.6789806978705594\n",
      "2022-12-04 02:45:43.024133 Epoch 118, Training Loss 0.6754480551956864\n",
      "2022-12-04 02:45:53.858654 Epoch 119, Training Loss 0.6758025144524586\n",
      "2022-12-04 02:46:04.710493 Epoch 120, Training Loss 0.6736303828561397\n",
      "2022-12-04 02:46:15.275574 Epoch 121, Training Loss 0.6740008556400724\n",
      "2022-12-04 02:46:26.109878 Epoch 122, Training Loss 0.6714085458642076\n",
      "2022-12-04 02:46:37.061446 Epoch 123, Training Loss 0.6694759944515765\n",
      "2022-12-04 02:46:47.998778 Epoch 124, Training Loss 0.668859529068403\n",
      "2022-12-04 02:46:58.840072 Epoch 125, Training Loss 0.667637961409281\n",
      "2022-12-04 02:47:09.579834 Epoch 126, Training Loss 0.6656535759072779\n",
      "2022-12-04 02:47:20.350196 Epoch 127, Training Loss 0.6649446499622081\n",
      "2022-12-04 02:47:31.370763 Epoch 128, Training Loss 0.6636270067423505\n",
      "2022-12-04 02:47:42.258972 Epoch 129, Training Loss 0.6616458740761822\n",
      "2022-12-04 02:47:54.447311 Epoch 130, Training Loss 0.659678631388318\n",
      "2022-12-04 02:48:05.424753 Epoch 131, Training Loss 0.659113749251951\n",
      "2022-12-04 02:48:16.218654 Epoch 132, Training Loss 0.6583795037568377\n",
      "2022-12-04 02:48:27.031816 Epoch 133, Training Loss 0.6569538333303179\n",
      "2022-12-04 02:48:37.715220 Epoch 134, Training Loss 0.655685971650626\n",
      "2022-12-04 02:48:48.378589 Epoch 135, Training Loss 0.6546555011702315\n",
      "2022-12-04 02:48:59.337903 Epoch 136, Training Loss 0.6542779870731447\n",
      "2022-12-04 02:49:10.309025 Epoch 137, Training Loss 0.6524341317927441\n",
      "2022-12-04 02:49:21.411637 Epoch 138, Training Loss 0.6517235989994405\n",
      "2022-12-04 02:49:32.177748 Epoch 139, Training Loss 0.6504195806620371\n",
      "2022-12-04 02:49:42.901421 Epoch 140, Training Loss 0.6493234733105315\n",
      "2022-12-04 02:49:53.918503 Epoch 141, Training Loss 0.6478987146369026\n",
      "2022-12-04 02:50:04.741947 Epoch 142, Training Loss 0.6466088784701379\n",
      "2022-12-04 02:50:16.525221 Epoch 143, Training Loss 0.644090808947068\n",
      "2022-12-04 02:50:27.400572 Epoch 144, Training Loss 0.6435407533136475\n",
      "2022-12-04 02:50:38.262107 Epoch 145, Training Loss 0.6421993664082359\n",
      "2022-12-04 02:50:49.087849 Epoch 146, Training Loss 0.6417080739041423\n",
      "2022-12-04 02:51:00.073478 Epoch 147, Training Loss 0.6407650280029268\n",
      "2022-12-04 02:51:10.943503 Epoch 148, Training Loss 0.6403663990366489\n",
      "2022-12-04 02:51:21.678765 Epoch 149, Training Loss 0.6388410142696727\n",
      "2022-12-04 02:51:32.575003 Epoch 150, Training Loss 0.6375932678618395\n",
      "2022-12-04 02:51:43.416489 Epoch 151, Training Loss 0.637352978596297\n",
      "2022-12-04 02:51:54.273270 Epoch 152, Training Loss 0.6343497072949129\n",
      "2022-12-04 02:52:05.168513 Epoch 153, Training Loss 0.634811586202563\n",
      "2022-12-04 02:52:15.938870 Epoch 154, Training Loss 0.6342136774526532\n",
      "2022-12-04 02:52:26.661736 Epoch 155, Training Loss 0.6329497179335646\n",
      "2022-12-04 02:52:38.473655 Epoch 156, Training Loss 0.6312201204507247\n",
      "2022-12-04 02:52:49.206279 Epoch 157, Training Loss 0.6300283424232317\n",
      "2022-12-04 02:53:00.084074 Epoch 158, Training Loss 0.6294289530085786\n",
      "2022-12-04 02:53:10.950164 Epoch 159, Training Loss 0.6288624386805708\n",
      "2022-12-04 02:53:21.732931 Epoch 160, Training Loss 0.627105299545371\n",
      "2022-12-04 02:53:32.538764 Epoch 161, Training Loss 0.6260626039770253\n",
      "2022-12-04 02:53:43.108575 Epoch 162, Training Loss 0.6239476609794076\n",
      "2022-12-04 02:53:53.771087 Epoch 163, Training Loss 0.6240958362207998\n",
      "2022-12-04 02:54:04.609070 Epoch 164, Training Loss 0.6234381010236643\n",
      "2022-12-04 02:54:15.291265 Epoch 165, Training Loss 0.6228326700456307\n",
      "2022-12-04 02:54:26.029637 Epoch 166, Training Loss 0.6201990055268073\n",
      "2022-12-04 02:54:36.901367 Epoch 167, Training Loss 0.6199999363602274\n",
      "2022-12-04 02:54:47.622974 Epoch 168, Training Loss 0.6184200024437112\n",
      "2022-12-04 02:54:59.190305 Epoch 169, Training Loss 0.6190985346313023\n",
      "2022-12-04 02:55:10.379946 Epoch 170, Training Loss 0.6168357469617863\n",
      "2022-12-04 02:55:21.058498 Epoch 171, Training Loss 0.6169751442378134\n",
      "2022-12-04 02:55:31.833534 Epoch 172, Training Loss 0.6147851288089972\n",
      "2022-12-04 02:55:42.624696 Epoch 173, Training Loss 0.6144534233585953\n",
      "2022-12-04 02:55:53.375248 Epoch 174, Training Loss 0.6144049916502154\n",
      "2022-12-04 02:56:04.250297 Epoch 175, Training Loss 0.6128296762552408\n",
      "2022-12-04 02:56:14.915849 Epoch 176, Training Loss 0.6106608492867721\n",
      "2022-12-04 02:56:25.549420 Epoch 177, Training Loss 0.6106827459905458\n",
      "2022-12-04 02:56:36.232115 Epoch 178, Training Loss 0.6098898675702417\n",
      "2022-12-04 02:56:46.916994 Epoch 179, Training Loss 0.6077375412368409\n",
      "2022-12-04 02:56:57.726959 Epoch 180, Training Loss 0.6075628137268374\n",
      "2022-12-04 02:57:08.594677 Epoch 181, Training Loss 0.6078900744009506\n",
      "2022-12-04 02:57:19.741902 Epoch 182, Training Loss 0.60631880812023\n",
      "2022-12-04 02:57:31.040240 Epoch 183, Training Loss 0.6063282684901791\n",
      "2022-12-04 02:57:41.682966 Epoch 184, Training Loss 0.6050588739345141\n",
      "2022-12-04 02:57:52.142694 Epoch 185, Training Loss 0.6053868433855989\n",
      "2022-12-04 02:58:03.063777 Epoch 186, Training Loss 0.603879672906283\n",
      "2022-12-04 02:58:13.884507 Epoch 187, Training Loss 0.6033439711879587\n",
      "2022-12-04 02:58:24.655369 Epoch 188, Training Loss 0.6031917758152613\n",
      "2022-12-04 02:58:35.409669 Epoch 189, Training Loss 0.6007741505608839\n",
      "2022-12-04 02:58:46.291257 Epoch 190, Training Loss 0.6006211506588685\n",
      "2022-12-04 02:58:57.080760 Epoch 191, Training Loss 0.598836944536175\n",
      "2022-12-04 02:59:07.831649 Epoch 192, Training Loss 0.5992551066000443\n",
      "2022-12-04 02:59:18.466759 Epoch 193, Training Loss 0.5971598015416919\n",
      "2022-12-04 02:59:29.128402 Epoch 194, Training Loss 0.597204035962634\n",
      "2022-12-04 02:59:39.823322 Epoch 195, Training Loss 0.5973804538969494\n",
      "2022-12-04 02:59:51.615168 Epoch 196, Training Loss 0.5950088512028575\n",
      "2022-12-04 03:00:02.524307 Epoch 197, Training Loss 0.5941902283588638\n",
      "2022-12-04 03:00:13.389544 Epoch 198, Training Loss 0.5939679368377646\n",
      "2022-12-04 03:00:24.116991 Epoch 199, Training Loss 0.5929106868746336\n",
      "2022-12-04 03:00:34.808928 Epoch 200, Training Loss 0.5937919846885954\n",
      "2022-12-04 03:00:45.613871 Epoch 201, Training Loss 0.5924714782734966\n",
      "2022-12-04 03:00:56.373705 Epoch 202, Training Loss 0.5905440984022282\n",
      "2022-12-04 03:01:07.188798 Epoch 203, Training Loss 0.5891456257580491\n",
      "2022-12-04 03:01:17.936450 Epoch 204, Training Loss 0.5902357581250198\n",
      "2022-12-04 03:01:28.598755 Epoch 205, Training Loss 0.5879473218413265\n",
      "2022-12-04 03:01:39.284533 Epoch 206, Training Loss 0.5872564124481757\n",
      "2022-12-04 03:01:49.788237 Epoch 207, Training Loss 0.5864096097933972\n",
      "2022-12-04 03:02:00.541317 Epoch 208, Training Loss 0.5855045479429347\n",
      "2022-12-04 03:02:11.946752 Epoch 209, Training Loss 0.5853254933034062\n",
      "2022-12-04 03:02:23.180673 Epoch 210, Training Loss 0.5851761167082945\n",
      "2022-12-04 03:02:33.793296 Epoch 211, Training Loss 0.5842550230376861\n",
      "2022-12-04 03:02:44.503642 Epoch 212, Training Loss 0.58286817741516\n",
      "2022-12-04 03:02:55.076952 Epoch 213, Training Loss 0.5829781269859475\n",
      "2022-12-04 03:03:05.778290 Epoch 214, Training Loss 0.5829927071052439\n",
      "2022-12-04 03:03:16.466711 Epoch 215, Training Loss 0.5817286484991498\n",
      "2022-12-04 03:03:27.080670 Epoch 216, Training Loss 0.5805536729981527\n",
      "2022-12-04 03:03:37.795723 Epoch 217, Training Loss 0.580471144041137\n",
      "2022-12-04 03:03:48.415270 Epoch 218, Training Loss 0.5802933242543579\n",
      "2022-12-04 03:03:59.146096 Epoch 219, Training Loss 0.57933069689347\n",
      "2022-12-04 03:04:09.878127 Epoch 220, Training Loss 0.5781953300127898\n",
      "2022-12-04 03:04:20.555627 Epoch 221, Training Loss 0.5767983717038808\n",
      "2022-12-04 03:04:31.123779 Epoch 222, Training Loss 0.5759941355689712\n",
      "2022-12-04 03:04:42.838018 Epoch 223, Training Loss 0.5755917128851956\n",
      "2022-12-04 03:04:53.340682 Epoch 224, Training Loss 0.5744970198863607\n",
      "2022-12-04 03:05:04.041519 Epoch 225, Training Loss 0.5744528641443118\n",
      "2022-12-04 03:05:14.797366 Epoch 226, Training Loss 0.5722709041650947\n",
      "2022-12-04 03:05:25.449035 Epoch 227, Training Loss 0.5742475444551014\n",
      "2022-12-04 03:05:36.031915 Epoch 228, Training Loss 0.5733143264222937\n",
      "2022-12-04 03:05:46.865547 Epoch 229, Training Loss 0.5727267648329211\n",
      "2022-12-04 03:05:57.594201 Epoch 230, Training Loss 0.572253772174306\n",
      "2022-12-04 03:06:08.435101 Epoch 231, Training Loss 0.5692959837520214\n",
      "2022-12-04 03:06:19.353910 Epoch 232, Training Loss 0.56984849030252\n",
      "2022-12-04 03:06:29.937285 Epoch 233, Training Loss 0.5680206020927185\n",
      "2022-12-04 03:06:40.655452 Epoch 234, Training Loss 0.5691189870543187\n",
      "2022-12-04 03:06:51.340741 Epoch 235, Training Loss 0.5675691902408819\n",
      "2022-12-04 03:07:02.052661 Epoch 236, Training Loss 0.5685601410506022\n",
      "2022-12-04 03:07:13.757582 Epoch 237, Training Loss 0.5669002788679679\n",
      "2022-12-04 03:07:24.418059 Epoch 238, Training Loss 0.5658695834600712\n",
      "2022-12-04 03:07:35.034386 Epoch 239, Training Loss 0.5650603306262999\n",
      "2022-12-04 03:07:45.619753 Epoch 240, Training Loss 0.5646367211399785\n",
      "2022-12-04 03:07:56.217911 Epoch 241, Training Loss 0.5646917951076537\n",
      "2022-12-04 03:08:06.934408 Epoch 242, Training Loss 0.563631559500609\n",
      "2022-12-04 03:08:17.718179 Epoch 243, Training Loss 0.5626174196829576\n",
      "2022-12-04 03:08:28.355884 Epoch 244, Training Loss 0.5642160320525889\n",
      "2022-12-04 03:08:39.128558 Epoch 245, Training Loss 0.5612010171498789\n",
      "2022-12-04 03:08:49.784255 Epoch 246, Training Loss 0.5622955612133226\n",
      "2022-12-04 03:09:00.403933 Epoch 247, Training Loss 0.5611456131081447\n",
      "2022-12-04 03:09:11.079108 Epoch 248, Training Loss 0.5624945053206686\n",
      "2022-12-04 03:09:21.750226 Epoch 249, Training Loss 0.5592206857355354\n",
      "2022-12-04 03:09:32.665844 Epoch 250, Training Loss 0.5593400568989537\n",
      "2022-12-04 03:09:43.919653 Epoch 251, Training Loss 0.5605626071200651\n",
      "2022-12-04 03:09:54.613681 Epoch 252, Training Loss 0.5571086706255403\n",
      "2022-12-04 03:10:05.299239 Epoch 253, Training Loss 0.5586023124511285\n",
      "2022-12-04 03:10:15.977380 Epoch 254, Training Loss 0.5565415961891794\n",
      "2022-12-04 03:10:26.701317 Epoch 255, Training Loss 0.5561611830730877\n",
      "2022-12-04 03:10:37.259126 Epoch 256, Training Loss 0.5579130264270641\n",
      "2022-12-04 03:10:47.916225 Epoch 257, Training Loss 0.5541262852261438\n",
      "2022-12-04 03:10:58.561933 Epoch 258, Training Loss 0.5555196656366749\n",
      "2022-12-04 03:11:09.134611 Epoch 259, Training Loss 0.5535762309837524\n",
      "2022-12-04 03:11:19.746415 Epoch 260, Training Loss 0.5536814952445457\n",
      "2022-12-04 03:11:30.338786 Epoch 261, Training Loss 0.5520804493933382\n",
      "2022-12-04 03:11:40.923353 Epoch 262, Training Loss 0.5540468455542384\n",
      "2022-12-04 03:11:51.743978 Epoch 263, Training Loss 0.5530087733283982\n",
      "2022-12-04 03:12:02.877093 Epoch 264, Training Loss 0.5497591816022268\n",
      "2022-12-04 03:12:14.150098 Epoch 265, Training Loss 0.5510173738764985\n",
      "2022-12-04 03:12:24.834663 Epoch 266, Training Loss 0.5503757274364267\n",
      "2022-12-04 03:12:35.447601 Epoch 267, Training Loss 0.5514154847892349\n",
      "2022-12-04 03:12:45.885268 Epoch 268, Training Loss 0.5485180357609258\n",
      "2022-12-04 03:12:56.528399 Epoch 269, Training Loss 0.549105311179405\n",
      "2022-12-04 03:13:07.264468 Epoch 270, Training Loss 0.5465142515385547\n",
      "2022-12-04 03:13:17.984281 Epoch 271, Training Loss 0.5463278998461221\n",
      "2022-12-04 03:13:28.844720 Epoch 272, Training Loss 0.548357236339613\n",
      "2022-12-04 03:13:39.543361 Epoch 273, Training Loss 0.5473054555599647\n",
      "2022-12-04 03:13:50.342144 Epoch 274, Training Loss 0.5454251614906599\n",
      "2022-12-04 03:14:01.090760 Epoch 275, Training Loss 0.5450365872829771\n",
      "2022-12-04 03:14:11.858097 Epoch 276, Training Loss 0.5452317092043665\n",
      "2022-12-04 03:14:22.628780 Epoch 277, Training Loss 0.544786841630021\n",
      "2022-12-04 03:14:34.502833 Epoch 278, Training Loss 0.5446557728264033\n",
      "2022-12-04 03:14:45.370935 Epoch 279, Training Loss 0.5431560944870609\n",
      "2022-12-04 03:14:56.232466 Epoch 280, Training Loss 0.5431312487634552\n",
      "2022-12-04 03:15:06.968124 Epoch 281, Training Loss 0.5437622986486196\n",
      "2022-12-04 03:15:17.724920 Epoch 282, Training Loss 0.5423144862779876\n",
      "2022-12-04 03:15:28.434621 Epoch 283, Training Loss 0.5414020749537841\n",
      "2022-12-04 03:15:39.052616 Epoch 284, Training Loss 0.5409174883533316\n",
      "2022-12-04 03:15:49.667069 Epoch 285, Training Loss 0.5412320665386327\n",
      "2022-12-04 03:16:00.463838 Epoch 286, Training Loss 0.5412564913330176\n",
      "2022-12-04 03:16:11.220253 Epoch 287, Training Loss 0.5406555252154465\n",
      "2022-12-04 03:16:21.990127 Epoch 288, Training Loss 0.5391334790326751\n",
      "2022-12-04 03:16:32.704860 Epoch 289, Training Loss 0.5397961688087419\n",
      "2022-12-04 03:16:43.337549 Epoch 290, Training Loss 0.5402134362693942\n",
      "2022-12-04 03:16:54.038408 Epoch 291, Training Loss 0.5379719682742873\n",
      "2022-12-04 03:17:05.898608 Epoch 292, Training Loss 0.539885512893767\n",
      "2022-12-04 03:17:16.689522 Epoch 293, Training Loss 0.5370795635692299\n",
      "2022-12-04 03:17:27.429860 Epoch 294, Training Loss 0.5397938363196905\n",
      "2022-12-04 03:17:38.054106 Epoch 295, Training Loss 0.5366068701343158\n",
      "2022-12-04 03:17:48.576637 Epoch 296, Training Loss 0.536600390053771\n",
      "2022-12-04 03:17:59.376463 Epoch 297, Training Loss 0.5364600496600046\n",
      "2022-12-04 03:18:10.288904 Epoch 298, Training Loss 0.5361874834693912\n",
      "2022-12-04 03:18:20.981532 Epoch 299, Training Loss 0.5355152020712033\n",
      "2022-12-04 03:18:31.747595 Epoch 300, Training Loss 0.5334769561505683\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, \n",
    "                    batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net().to('cuda:0')\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs =300,\n",
    "    optimizer= optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    \n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
